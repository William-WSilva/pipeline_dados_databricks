{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db354d1-c339-40b0-84c0-44788e03808e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, to_date, regexp_replace\n",
    "from pyspark.sql.types import IntegerType, DecimalType, DateType\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n",
    "\n",
    "def clean_and_transform_employees(file_path):\n",
    "    \"\"\"\n",
    "    Limpa e transforma os dados dos funcionários, removendo duplicatas, espaços em branco, e garantindo tipos de dados corretos.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Caminho para o arquivo JSON contendo os dados dos funcionários.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Lê o arquivo JSON\n",
    "    df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "    \n",
    "    if \"id\" not in df.columns:\n",
    "        print(\"A coluna 'id' não foi encontrada no DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    # Remover espaços em branco\n",
    "    df = df.withColumn(\"name\", trim(col(\"name\")))\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    df = df.dropDuplicates(['id'])\n",
    "    \n",
    "    # Converter 'id' para Integer\n",
    "    df = df.withColumn(\"id\", col(\"id\").cast(IntegerType()))\n",
    "    \n",
    "    # Escrever os dados transformados no formato Parquet\n",
    "    df.write.mode('overwrite').parquet(\"/mnt/processing/employees_data.parquet\")\n",
    "    print(\"Dados de funcionários processados e salvos na camada processing.\")\n",
    "\n",
    "def clean_and_transform_sales(file_path):\n",
    "    \"\"\"\n",
    "    Limpa e transforma os dados de vendas, removendo duplicatas, garantindo tipos de dados corretos e formatando datas.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Caminho para o arquivo CSV contendo os dados de vendas.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    df = df.dropDuplicates(['id_venda'])\n",
    "    \n",
    "    # Remover espaços em branco\n",
    "    df = df.withColumn(\"id_funcionario\", trim(col(\"id_funcionario\")))\n",
    "    df = df.withColumn(\"id_categoria\", trim(col(\"id_categoria\")))\n",
    "    \n",
    "    # Converter 'id_funcionario' e 'id_categoria' para Integer\n",
    "    df = df.withColumn(\"id_venda\", col(\"id_venda\").cast(IntegerType()))\n",
    "    df = df.withColumn(\"id_funcionario\", col(\"id_funcionario\").cast(IntegerType()))\n",
    "    df = df.withColumn(\"id_categoria\", col(\"id_categoria\").cast(IntegerType()))\n",
    "    \n",
    "    # Garantir que a coluna 'data_venda' está no formato de data\n",
    "    df = df.withColumn(\"data_venda\", to_date(col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Converter 'venda' para Integer (ou Decimal, se necessário)\n",
    "    df = df.withColumn(\"venda\", regexp_replace(col(\"venda\"), \",\", \"\").cast(DecimalType(10, 2)))\n",
    "    \n",
    "    # Escrever os dados transformados no formato Parquet\n",
    "    df.write.mode('overwrite').parquet(\"/mnt/processing/sales_data.parquet\")\n",
    "    print(\"Dados de vendas processados e salvos na camada processing.\")\n",
    "\n",
    "def clean_and_transform_categories(file_path):\n",
    "    \"\"\"\n",
    "    Limpa e transforma os dados de categorias, removendo duplicatas e espaços em branco.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Caminho para o arquivo Parquet contendo os dados de categorias.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    df = df.dropDuplicates(['id'])\n",
    "    \n",
    "    # Remover espaços em branco\n",
    "    df = df.withColumn(\"nome_categoria\", trim(col(\"nome_categoria\")))\n",
    "    \n",
    "    # Converter 'id' para Integer\n",
    "    df = df.withColumn(\"id\", col(\"id\").cast(IntegerType()))\n",
    "    \n",
    "    # Escrever os dados transformados no formato Parquet\n",
    "    df.write.mode('overwrite').parquet(\"/mnt/processing/categories_data.parquet\")\n",
    "    print(\"Dados de categorias processados e salvos na camada processing.\")\n",
    "\n",
    "# Caminhos para os arquivos na landing\n",
    "employees_json_path = \"/mnt/landing/employees_data.json\"\n",
    "sales_csv_path = \"/mnt/landing/sales_data.csv\"\n",
    "categories_parquet_path = \"/mnt/landing/categoria.parquet\"\n",
    "\n",
    "# Chamar as funções de processamento\n",
    "clean_and_transform_employees(employees_json_path)\n",
    "clean_and_transform_sales(sales_csv_path)\n",
    "clean_and_transform_categories(categories_parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b164d3-0c94-4c91-a597-09a7d037a428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar arquivos no container 'processing'\n",
    "files = dbutils.fs.ls(\"/mnt/processing/\")\n",
    "for file in files:\n",
    "    print(file.path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_to_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
