{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1c6af5-1767-4d28-8b09-181f0a140044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "client_id = dbutils.secrets.get('bix_data_scope', 'client-id')\n",
    "client_secret = dbutils.secrets.get('bix_data_scope', 'client-secret')\n",
    "endpoint_id = dbutils.secrets.get('bix_data_scope', 'endpoint-id')\n",
    "\n",
    "def configure_mounts(spark, configs, mounts):\n",
    "    \"\"\"\n",
    "    Configura os pontos de montagem no Databricks, desmontando se já estiverem montados e montando novamente com as novas configurações.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão ativa do Spark.\n",
    "        configs (dict): Dicionário com as configurações de autenticação para o Azure Data Lake Storage.\n",
    "        mounts (dict): Dicionário contendo os pontos de montagem e seus respectivos caminhos de origem no Azure.\n",
    "    \"\"\"\n",
    "    # Obter os pontos de montagem atuais\n",
    "    mounts_df = spark.createDataFrame(dbutils.fs.mounts())\n",
    "    \n",
    "    # Iterar sobre os pontos de montagem e configurar cada um\n",
    "    for mount_point, source in mounts.items():\n",
    "        if mounts_df.filter(mounts_df.mountPoint == mount_point).count() > 0:\n",
    "            # Desmontar se já estiver montado\n",
    "            dbutils.fs.unmount(mount_point)\n",
    "        \n",
    "        # Montar novamente com as configurações fornecidas\n",
    "        dbutils.fs.mount(\n",
    "            source=source,\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "    print(\"Configurações de montagem concluídas.\")\n",
    "\n",
    "# Sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"BixDataPipeline\").getOrCreate()\n",
    "\n",
    "# Configurações de autenticação para o Azure Data Lake Storage\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": f\"{client_id}\", \n",
    "    \"fs.azure.account.oauth2.client.secret\": f\"{client_secret}\",\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"{endpoint_id}\"\n",
    "}\n",
    "\n",
    "# Dicionário com as informações dos containers e pontos de montagem\n",
    "mounts = {\n",
    "    \"/mnt/landing\": \"abfss://landing@projbixarmazenamento.dfs.core.windows.net/\",\n",
    "    \"/mnt/processing\": \"abfss://processing@projbixarmazenamento.dfs.core.windows.net/\",\n",
    "    \"/mnt/curated\": \"abfss://curated@projbixarmazenamento.dfs.core.windows.net/\"\n",
    "}\n",
    "\n",
    "# Configurar as montagens\n",
    "configure_mounts(spark, configs, mounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf3179f-8407-4628-b7bd-697d7f5548d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# URL da API dos funcionários\n",
    "api_url = \"https://us-central1-bix-tecnologia-prd.cloudfunctions.net/api_challenge_junior\"\n",
    "\n",
    "def extract_employees(api_url, start_id, end_id):\n",
    "    \"\"\"\n",
    "    Extrai os dados dos funcionários a partir da API.\n",
    "\n",
    "    Args:\n",
    "        api_url (str): URL da API para extração dos dados.\n",
    "        start_id (int): ID inicial do funcionário.\n",
    "        end_id (int): ID final do funcionário.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for employee_id in range(start_id, end_id + 1):\n",
    "        url = f\"{api_url}?id={employee_id}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.text.strip()\n",
    "            all_data.append({\"id\": employee_id, \"name\": data})\n",
    "        else:\n",
    "            print(f\"Falha na obtenção de dados para ID {employee_id}: {response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "    return all_data\n",
    "\n",
    "def save_employees_to_landing(file_name, employees_data):\n",
    "    \"\"\"\n",
    "    Salva os dados dos funcionários em formato JSON na camada 'landing'.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Nome do arquivo JSON a ser salvo.\n",
    "        employees_data (list): Lista de dicionários contendo os dados dos funcionários.\n",
    "    \"\"\"\n",
    "    file_path = f\"/mnt/landing/{file_name}\"\n",
    "    \n",
    "    # Converte os dados para formato JSON\n",
    "    json_data = json.dumps(employees_data, indent=4)\n",
    "    \n",
    "    # Salva os dados em formato JSON no DBFS\n",
    "    try:\n",
    "        dbutils.fs.put(file_path, json_data, overwrite=True)\n",
    "        print(f\"Arquivo JSON '{file_name}' salvo com sucesso em /mnt/landing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o arquivo JSON: {e}\")\n",
    "\n",
    "\n",
    "employees_data = extract_employees(api_url, 1, 9)\n",
    "save_employees_to_landing(\"employees_data.json\", employees_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b97da8c-1348-411e-82bf-d7bffb6062d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# URL do arquivo Parquet e nome do arquivo\n",
    "parquet_url = \"https://storage.googleapis.com/challenge_junior/categoria.parquet\"\n",
    "file_name = \"categoria.parquet\"\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"DownloadParquet\").getOrCreate()\n",
    "\n",
    "def save_parquet_to_landing(parquet_url, file_name):\n",
    "    \"\"\"\n",
    "    Baixa um arquivo Parquet da URL fornecida e o salva na pasta /mnt/landing.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL do arquivo Parquet.\n",
    "        file_name (str): Nome do arquivo Parquet a ser salvo.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    landing_dir = \"/mnt/landing\"\n",
    "    temp_file_path = f\"/tmp/{file_name}\"\n",
    "    dbfs_temp_path = f\"dbfs:/tmp/{file_name}\"\n",
    "    destination_path = os.path.join(landing_dir, file_name)\n",
    "    \n",
    "    try:\n",
    "        # Baixa o arquivo Parquet da URL\n",
    "        response = requests.get(parquet_url)\n",
    "        if response.status_code == 200:\n",
    "            # Salva o arquivo Parquet temporariamente no sistema local\n",
    "            with open(temp_file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Arquivo Parquet baixado e salvo temporariamente em {temp_file_path}\")\n",
    "            \n",
    "            # Copia o arquivo Parquet para o DBFS\n",
    "            dbutils.fs.cp(f\"file:{temp_file_path}\", dbfs_temp_path)\n",
    "            print(f\"Arquivo Parquet copiado para o DBFS em {dbfs_temp_path}\")\n",
    "            \n",
    "            # Lê o arquivo Parquet no DBFS em um DataFrame do PySpark\n",
    "            df = spark.read.parquet(dbfs_temp_path)\n",
    "            \n",
    "            # Salva o DataFrame como Parquet no DBFS na pasta /mnt/landing\n",
    "            df.write.mode(\"overwrite\").parquet(destination_path)\n",
    "            print(f\"Arquivo Parquet salvo em {destination_path}\")\n",
    "            \n",
    "            # Remove o arquivo temporário tanto no local quanto no DBFS\n",
    "            os.remove(temp_file_path)\n",
    "            dbutils.fs.rm(dbfs_temp_path)\n",
    "        else:\n",
    "            print(f\"Falha ao baixar o arquivo Parquet: {response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o arquivo Parquet: {e}\")\n",
    "\n",
    "\n",
    "save_parquet_to_landing(parquet_url, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4deafb2d-dd8a-4305-ae48-7830ee2bad9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações para conectar ao banco de dados PostgreSQL\n",
    "postgres_host = dbutils.secrets.get('bix_data_scope', 'postgres-host')\n",
    "postgres_user = dbutils.secrets.get('bix_data_scope', 'postgres-user')\n",
    "postgres_password = dbutils.secrets.get('bix_data_scope', 'postgres-password')\n",
    "postgres_port = dbutils.secrets.get('bix_data_scope', 'postgres-port')\n",
    "postgres_db = dbutils.secrets.get('bix_data_scope', 'postgres-db')\n",
    "\n",
    "postgres_conn = {\n",
    "    \"host\": postgres_host,\n",
    "    \"user\": postgres_user,\n",
    "    \"password\": postgres_password,\n",
    "    \"port\": postgres_port,\n",
    "    \"dbname\": postgres_db\n",
    "}\n",
    "\n",
    "\n",
    "def extract_and_save_sales_data(postgres_conn, csv_path):\n",
    "    \"\"\"\n",
    "    Extrai dados da tabela de vendas do banco de dados PostgreSQL e os salva como um arquivo CSV.\n",
    "\n",
    "    Args:\n",
    "        postgres_conn (dict): Dicionário com os parâmetros de conexão ao banco de dados PostgreSQL.\n",
    "        csv_path (str): Caminho no DBFS onde o arquivo CSV será salvo.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        # Conectar ao banco de dados PostgreSQL\n",
    "        conn = psycopg2.connect(**postgres_conn)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Executar consulta SQL para extrair dados\n",
    "        query_sales = sql.SQL(\"SELECT * FROM public.venda\")\n",
    "        cursor.execute(query_sales)\n",
    "        rows = cursor.fetchall()\n",
    "        col_names = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Criar DataFrame com os dados extraídos\n",
    "        df = pd.DataFrame(rows, columns=col_names)\n",
    "        \n",
    "        # Salvar o DataFrame como um arquivo CSV localmente\n",
    "        local_csv_path = \"/tmp/sales_data.csv\"\n",
    "        df.to_csv(local_csv_path, index=False, header=True)\n",
    "        print(f\"Dados salvos como CSV localmente em {local_csv_path}\")\n",
    "\n",
    "        # Copiar o arquivo CSV para o DBFS\n",
    "        dbfs_csv_path = f\"{csv_path}\"\n",
    "        dbutils.fs.cp(f\"file:{local_csv_path}\", dbfs_csv_path)\n",
    "        print(f\"Arquivo CSV copiado para o DBFS em {dbfs_csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair e salvar dados do banco de dados: {e}\")\n",
    "    finally:\n",
    "        # Fechar o cursor e a conexão com o banco de dados\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Caminho do arquivo CSV na pasta /mnt/landing\n",
    "csv_path = \"/mnt/landing/sales_data.csv\"\n",
    "\n",
    "\n",
    "extract_and_save_sales_data(postgres_conn, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "620b836a-ee79-4b1e-9a69-b11b9bdccc70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar arquivos no container 'landing'\n",
    "files = dbutils.fs.ls(\"/mnt/landing/\")\n",
    "for file in files:\n",
    "    print(file.path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_to_landing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
